{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to import modules from src\n",
    "rpath = os.path.abspath('..')\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)\n",
    "\n",
    "from src.loader import SlackDataLoader\n",
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataLoader\n",
    "sl = SlackDataLoader('../Anonymized_B6SlackExport_25Nov23/anonymized/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns we can get from a slack message<br>\n",
    "\n",
    "message_type, message_content, sender_id, time_sent, message_distribution, time_thread_start, reply_count, reply_user_count, time_thread_end, reply_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a single slack message, we can get <br>\n",
    "\n",
    "1. The message<br>\n",
    "2. Type (message, file, link, etc)<br>\n",
    "3. The sender_id (assigned by slack)<br>\n",
    "4. The time the message was sent<br>\n",
    "5. The team (i don't know what that is now)<br>\n",
    "6. The type of the message (broadcast message, inhouse, just messgae)<br>\n",
    "7. The thread the message generated (from here we can go):<br>\n",
    "    7.1 Text/content of the message<br>\n",
    "    7.2 The thread time of the message<br>\n",
    "    7.3 The thread count (reply count)<br>\n",
    "    7.4 The number of user that reply the message (count of users that participated in the thread)<br>\n",
    "    7.5 The time the last thread message was sent <br>\n",
    "    7.6 The users that participated in the thread (their ids are stored as well)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_20_user(data, channel='Random'):\n",
    "    \"\"\"get user with the highest number of message sent to any channel\"\"\"\n",
    "\n",
    "    data['sender_name'].value_counts()[:20].plot.bar(figsize=(15, 7.5))\n",
    "    plt.title(f'Top 20 Message Senders in #{channel} channels', size=15, fontweight='bold')\n",
    "    plt.xlabel(\"Sender Name\", size=18); plt.ylabel(\"Frequency\", size=14);\n",
    "    plt.xticks(size=12); plt.yticks(size=12);\n",
    "    plt.show()\n",
    "\n",
    "    data['sender_name'].value_counts()[-10:].plot.bar(figsize=(15, 7.5))\n",
    "    plt.title(f'Bottom 10 Message Senders in #{channel} channels', size=15, fontweight='bold')\n",
    "    plt.xlabel(\"Sender Name\", size=18); plt.ylabel(\"Frequency\", size=14);\n",
    "    plt.xticks(size=12); plt.yticks(size=12);\n",
    "    plt.show()\n",
    "\n",
    "def draw_avg_reply_count(data, channel='Random'):\n",
    "    \"\"\"who commands many reply?\"\"\"\n",
    "\n",
    "    data.groupby('sender_name')['reply_count'].mean().sort_values(ascending=False)[:20]\\\n",
    "        .plot(kind='bar', figsize=(15,7.5));\n",
    "    plt.title(f'Average Number of reply count per Sender in #{channel}', size=20, fontweight='bold')\n",
    "    plt.xlabel(\"Sender Name\", size=18); plt.ylabel(\"Frequency\", size=18);\n",
    "    plt.xticks(size=14); plt.yticks(size=14);\n",
    "    plt.show()\n",
    "\n",
    "def draw_avg_reply_users_count(data, channel='Random'):\n",
    "    \"\"\"who commands many user reply?\"\"\"\n",
    "\n",
    "    data.groupby('sender_name')['reply_users_count'].mean().sort_values(ascending=False)[:20].plot(kind='bar',\n",
    "     figsize=(15,7.5));\n",
    "    plt.title(f'Average Number of reply user count per Sender in #{channel}', size=20, fontweight='bold')\n",
    "    plt.xlabel(\"Sender Name\", size=18); plt.ylabel(\"Frequency\", size=18);\n",
    "    plt.xticks(size=14); plt.yticks(size=14);\n",
    "    plt.show()\n",
    "\n",
    "def draw_wordcloud(msg_content, week):    \n",
    "    # word cloud visualization\n",
    "    allWords = ' '.join([twts for twts in msg_content])\n",
    "    wordCloud = WordCloud(background_color='#975429', width=500, height=300, random_state=21, max_words=500, mode='RGBA',\n",
    "                            max_font_size=140, stopwords=stopwords.words('english')).generate(allWords)\n",
    "    plt.figure(figsize=(15, 7.5))\n",
    "    plt.imshow(wordCloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.title(f'WordCloud for {week}', size=30)\n",
    "    plt.show()\n",
    "\n",
    "def draw_user_reaction(data, channel='General'):\n",
    "    data.groupby('sender_name')[['reply_count', 'reply_users_count']].sum()\\\n",
    "        .sort_values(by='reply_count',ascending=False)[:10].plot(kind='bar', figsize=(15, 7.5))\n",
    "    plt.title(f'User with the most reaction in #{channel}', size=25);\n",
    "    plt.xlabel(\"Sender Name\", size=18); plt.ylabel(\"Frequency\", size=18);\n",
    "    plt.xticks(size=14); plt.yticks(size=14);\n",
    "    plt.show()\n",
    "def top_10_senders_eda(df):\n",
    "\n",
    "    # Find the top 10 senders by maximum reply_count\n",
    "    top_10_senders = df.groupby('sender_name')['reply_count'].max().nlargest(10).reset_index()\n",
    "\n",
    "    # Plot a bar chart for the top 10 senders by maximum reply_count\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='sender_name', y='reply_count', data=top_10_senders)\n",
    "    plt.title('Top 10 Senders by Maximum Reply Count')\n",
    "    plt.xlabel('Sender Name')\n",
    "    plt.ylabel('Maximum Reply Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "def bottom_10_senders_eda(df):\n",
    "\n",
    "    # Find the top 10 senders by maximum reply_count\n",
    "    bottom_10_senders_eda = df.groupby('sender_name')['reply_count'].min().nsmallest(10).reset_index()\n",
    "\n",
    "    # Plot a bar chart for the top 10 senders by maximum reply_count\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='sender_name', y='reply_count', data=bottom_10_senders_eda)\n",
    "    plt.title('Bottom 10 Senders by Maximum Reply Count')\n",
    "    plt.xlabel('Sender Name')\n",
    "    plt.ylabel('Maximum Reply Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight Extraction\n",
    "\n",
    "Below are some useful questions to answer. Feel free to explore to answer other interesting questions that may be of help to get insight about student's behaviour, need, and future performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting all json files for all-technical-support channel to dataframe using slack_parser method from loader script\n",
    "df = sdl.slack_parser('../Anonymized_B6SlackExport_25Nov23/anonymized/all-technical-support/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get user with the highest number of message sent to any channel\n",
    "get_top_20_user(df, channel='all-technical-support')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of reply count per sender\n",
    "draw_avg_reply_count(df, channel='all-technical-support')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_wordcloud(msg_content, week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_content = sdl.get_channel_messages(channel_name='all-technical-support')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# which user has the highest number of reply counts?\n",
    "The premise for the above question exploratory data analysis (EDA) assumes that reply counts are tallied individually for each message and are not aggregated on a per-user basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the maximum reply_count\n",
    "max_reply_count= df['reply_count'].max()\n",
    "max_reply_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the maximum reply_count\n",
    "max_reply_index = df['reply_count'].idxmax()\n",
    "\n",
    "# Get the user with the maximum reply_count\n",
    "user_with_max_reply = df.loc[max_reply_index, 'sender_name']\n",
    "\n",
    "print(f\"The user with the maximum reply_count is: {user_with_max_reply}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top and bottom 10 users by Reply count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 10 senders by maximum reply_count\n",
    "top_10_senders = df.groupby('sender_name')['reply_count'].max().nlargest(10).reset_index()\n",
    "# top_10_users = df.nlargest(10, 'reply_count')\n",
    "print(\"Top 10 users by reply_count:\")\n",
    "print(top_10_senders[['sender_name', 'reply_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom 10 users by reply_count\n",
    "bottom_10_users = df.groupby('sender_name')['reply_count'].min().nsmallest(10).reset_index()\n",
    "print(\"\\nBottom 10 users by reply_count:\")\n",
    "print(bottom_10_users[['sender_name', 'reply_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram plot for bottom_10_users\n",
    "bottom_10_senders_eda(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram plot for bottom_10_users\n",
    "top_10_senders_eda(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize reply counts per user per channel\n",
    "The premise for the above question exploratory data analysis (EDA) assumes that reply counts are aggregated on a per-user basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize it for all-technical-support channel\n",
    "pivot_df = df.pivot_table(index='sender_name', values='reply_count', aggfunc='sum', fill_value=0)\n",
    "\n",
    "# Plot a heatmap\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(pivotnltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to classify messages\n",
    "def classify_message(msg):\n",
    "    # Tokenize the message\n",
    "    words = word_tokenize(msg.lower())  # Convert to lowercase for case-insensitivity\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.isalnum() and word not in stopwords.words('english')]\n",
    "    \n",
    "    # Example classification logic (customize based on your needs)\n",
    "    if '?' in filtered_words:\n",
    "        return 'Question'\n",
    "    elif 'thanks' in filtered_words or 'thank you' in filtered_words:\n",
    "        return 'Thanks'\n",
    "    elif 'nice' in filtered_words or 'good job' in filtered_words:\n",
    "        return 'Appreciation'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the classification function to each message in the DataFrame\n",
    "df['message_category'] = df['msg_content'].apply(classify_message)\n",
    "\n",
    "# Display the DataFrame with the added 'message_category' column\n",
    "print(df)\n",
    "\n",
    "# Plot a word cloud for each category\n",
    "for category in df['message_category'].unique():\n",
    "    subset = df[df['message_category'] == category]\n",
    "    all_words = ' '.join(subset['msg_content'])\n",
    "    \n",
    "    wordcloud = WordCloud(width=500, height=300, background_color='white').generate(all_words)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f'Word Cloud for {category}')\n",
    "    plt.axis('off')\n",
    "    plt.show()_df, cmap='viridis', annot=True, fmt='d', linewidths=.5)\n",
    "plt.title('Reply Counts per User for Channel all-technical-support')\n",
    "plt.xlabel('User')\n",
    "plt.ylabel('Reply Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is the time range of the day that most messages are sent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'msg_sent_time' to datetime with Unix timestamp format\n",
    "df['msg_sent_time'] = pd.to_datetime(df['msg_sent_time'], unit='s')\n",
    "\n",
    "# Extract the hour from the 'msg_sent_time' column\n",
    "df['hour'] = df['msg_sent_time'].dt.hour\n",
    "\n",
    "# Plot a histogram to visualize the distribution of messages across hours\n",
    "plt.figure(figsize=(12, 6))\n",
    "df['hour'].plot(kind='hist', bins=24, rwidth=0.8, color='#86bf91', alpha=0.7)\n",
    "plt.title('Distribution of Messages Across Hours of the Day')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Number of Messages')\n",
    "plt.xticks(range(24))\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Find the hour range with the highest message count\n",
    "most_active_hour_range = df['hour'].value_counts().idxmax()\n",
    "print(f\"The time range of the day with the most messages is approximately {most_active_hour_range}:00 to {(most_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what kind of messages are replied faster than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'msg_sent_time' and 'tm_thread_end' to datetime\n",
    "df['msg_sent_time'] = pd.to_datetime(df['msg_sent_time'], unit='s')\n",
    "df['time_thread_start'] = pd.to_datetime(df['time_thread_start'], unit='s')\n",
    "\n",
    "# Calculate the time taken to reply to each message\n",
    "df['reply_time'] = (df['time_thread_start'] - df['msg_sent_time']).dt.total_seconds()\n",
    "\n",
    "average_reply_time_per_type = df.groupby('msg_type')['reply_time'].mean()\n",
    "\n",
    "# Plot a bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "average_reply_time_per_type.plot(kind='bar', color='#86bf91', alpha=0.7)\n",
    "plt.title('Average Reply Time per Message Type')\n",
    "plt.xlabel('Message Type')\n",
    "plt.ylabel('Average Reply Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unfortunately on this channel the message type is only message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between # of messages and # of reactions\n",
    "\n",
    "    \n",
    "df_reaction = sdl.parse_slack_reaction(channel='all-technical-support',path_channel='../Anonymized_B6SlackExport_25Nov23/anonymized/all-technical-support/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reaction.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify messages into different categories such as questions, answers, comments, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify messages\n",
    "def classify_message(msg):\n",
    "    # Tokenize the message\n",
    "    words = word_tokenize(msg.lower())  # Convert to lowercase for case-insensitivity\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.isalnum() and word not in stopwords.words('english')]\n",
    "    \n",
    "    # Example classification logic (customize based on your needs)\n",
    "    if '?' in filtered_words:\n",
    "        return 'Question'\n",
    "    elif 'thanks' in filtered_words or 'thank you' in filtered_words:\n",
    "        return 'Thanks'\n",
    "    elif 'nice' in filtered_words or 'good job' in filtered_words:\n",
    "        return 'Appreciation'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Apply the classification function to each message in the DataFrame\n",
    "df['message_category'] = df['msg_content'].apply(classify_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = df['message_category'].value_counts()\n",
    "\n",
    "# Plot a bar chart\n",
    "category_counts.plot(kind='bar', color='skyblue')\n",
    "plt.xlabel('Message Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Message Category Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which users got the most reactions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reactions = df_reaction.groupby('user_id')['reaction_count'].sum()\n",
    "\n",
    "# Sort the results to get the users with the most reactions\n",
    "user_reactions = user_reactions.sort_values(ascending=False)\n",
    "\n",
    "# Display the user_id and reaction_count for the top users\n",
    "top_users_reactions = user_reactions.reset_index()\n",
    "print(top_users_reactions[['user_id', 'reaction_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_name_dict = sdl.get_user_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the dictionary within the tuple\n",
    "user_id_name_dict = user_id_name_dict[0]\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df_users = pd.DataFrame(list(user_id_name_dict.items()), columns=['user_id', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames on the 'user_id' column\n",
    "df_merged = pd.merge(top_users_reactions, df_users, on='user_id')\n",
    "\n",
    "# Print the merged DataFrame\n",
    "print(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df_merged['name'], df_merged['reaction_count'], color='skyblue')\n",
    "plt.xlabel('User Name')\n",
    "plt.ylabel('Reaction Count')\n",
    "plt.title('Reaction Counts by User')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model topics mentioned in the channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Assuming 'message' is the column containing text data\n",
    "text_data = df_reaction['message'].astype(str)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = [word_tokenize(text.lower()) for text in text_data]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_text = [[word for word in text if word.isalnum() and word not in stop_words] for text in tokenized_text]\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(filtered_text)\n",
    "\n",
    "# Create a bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(text) for text in filtered_text]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics\n",
    "for topic_id, topic in lda_model.print_topics():\n",
    "    print(f'Topic {topic_id + 1}: {topic}')\n",
    "\n",
    "# Get the topic distribution for a specific document\n",
    "document_index = 0  # Change this to the index of the document you want to analyze\n",
    "doc_topics = lda_model.get_document_topics(corpus[document_index])\n",
    "print(f'Topic distribution for document {document_index + 1}: {doc_topics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the topics that got the most reactions?\n",
    "\n",
    "reaction_counts = df_reaction['reaction_count']\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = [word_tokenize(text.lower()) for text in text_data]\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_text = [[word for word in text if word.isalnum() and word not in stop_words] for text in tokenized_text]\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(filtered_text)\n",
    "\n",
    "# Create a bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(text) for text in filtered_text]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "# Create a DataFrame to store the topic distribution for each document\n",
    "topic_distribution = pd.DataFrame(lda_model[corpus], columns=[f'Topic_{i+1}' for i in range(lda_model.num_topics)])\n",
    "\n",
    "# Concatenate the topic distribution DataFrame with the original DataFrame\n",
    "df_with_topics = pd.concat([df_reaction, topic_distribution], axis=1)\n",
    "\n",
    "# Group by the topic columns and sum the reaction counts\n",
    "topic_reaction_counts = df_with_topics.groupby(topic_distribution.columns.tolist(), as_index=False)['reaction_count'].sum()\n",
    "\n",
    "# Print the topics and their associated reaction counts\n",
    "for index, row in topic_reaction_counts.iterrows():\n",
    "    print(f'Topic {row[topic_distribution.columns.tolist()]}: {row[\"reaction_count\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harder questions to look into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on messages, reactions, references shared, and other relevant data such as classification of questions into techical question, comment, answer, aorder stu the python, statistics, and sql skill level of a user?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
